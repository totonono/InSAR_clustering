{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SET UP, FULL EXAMPLE BASED ON df_asc_north"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import percentile\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import seaborn as sns; sns.set(font_scale=1.2) \n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn import metrics\n",
    "import pyod \n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.knn import KNN\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data LOADING,CLEANING, FORMATING & VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Data\n",
    "\n",
    "dataSUBSETan=pd.read_csv(\"C:/599_Research/FINAL_RESEARCH_and_PPT/THESIS_SUBMISSION/APPENDIX/3_MULTIPLE_ATTRIBUTE_SCRIPT/DATA/Merge_NORTH_A_SUBSET.csv\")\n",
    "\n",
    "dataSUBSETan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If dataset is covering many tiles we  can combine csv into 1 - we do this for ascending and descending and north and south\n",
    "#df_asc_south = pd.concat([data1as, data2as, data3as, data4as, data5as, data6as], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect - dataSUBSETan\n",
    "#dataSUBSETan.head()\n",
    "dataSUBSETan.describe()\n",
    "#dataSUBSETan.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually set CRS (it might work without depending on machine, but just in case)\n",
    "dataSUBSETan.crs = {'init': u'epsg:4326'}\n",
    "\n",
    "dataSUBSETan.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the data for the VEL(OR ANY OTHER ATTRIBUTE), changing the hue allows you to visualize any attribute\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.scatter(dataSUBSETan['LONG'],dataSUBSETan['LAT'], c= dataSUBSETan['VEL'], s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of any columns of information that are not needed ex below.(this saves on processing time)\n",
    "#dataSUBSETan.drop(['D20191213','D20200126'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate correct number of Clusters to Begin Looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###specific to GMM!\n",
    "#GMM\n",
    "#the Akaike information criterion (AIC) or the Bayesian information criterion (BIC).\n",
    "X = np.array(list(zip(dataSUBSETan['VEL'],dataSUBSETan['V_STDEV'])))\n",
    "n_components = np.arange(1, 21)\n",
    "models = [GMM(n, covariance_type='full', random_state=0).fit(X) for n in n_components]\n",
    "plt.plot(n_components, [m.bic(X) for m in models], label='BIC')\n",
    "plt.plot(n_components, [m.aic(X) for m in models], label='AIC')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('n_components');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SelBest(arr:list, X:int)->list:\n",
    "    '''\n",
    "    returns the set of X configurations with shorter distance\n",
    "    '''\n",
    "    dx=np.argsort(arr)[:X]\n",
    "    return arr[dx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Silhouette Score\n",
    "X = np.array(list(zip(dataSUBSETan['VEL'],dataSUBSETan['V_STDEV'])))\n",
    "n_clusters=np.arange(2, 20)\n",
    "sils=[]\n",
    "sils_err=[]\n",
    "iterations=20\n",
    "for n in n_clusters:\n",
    "    tmp_sil=[]\n",
    "    for _ in range(iterations):\n",
    "        gmm=GMM(n, n_init=2).fit(X) \n",
    "        labels=gmm.predict(X)\n",
    "        sil=metrics.silhouette_score(X, labels, metric='euclidean')\n",
    "        tmp_sil.append(sil)\n",
    "    val=np.mean(SelBest(np.array(tmp_sil), int(iterations/5)))\n",
    "    err=np.std(tmp_sil)\n",
    "    sils.append(val)\n",
    "    sils_err.append(err)\n",
    "    \n",
    "plt.errorbar(n_clusters, sils, yerr=sils_err)\n",
    "plt.title(\"Silhouette Scores\", fontsize=20)\n",
    "plt.xticks(n_clusters)\n",
    "plt.xlabel(\"N. of clusters\")\n",
    "plt.ylabel(\"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient score\n",
    "plt.errorbar(n_clusters, np.gradient(bics), yerr=bics_err, label='BIC')\n",
    "plt.title(\"Gradient of BIC Scores\", fontsize=20)\n",
    "plt.xticks(n_clusters)\n",
    "plt.xlabel(\"N. of clusters\")\n",
    "plt.ylabel(\"grad(BIC)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE SINGLE ATTRIBUTE SUBPLOTS AND CORRELATION MATRIX TO FIND THE BEST COMBO OF ATTRIBUTES TO USE FOR MODEL ASSESSMENT\n",
    "cluster = ['VEL','V_STDEV','ACC','COHERENCE','STD_DEF','SEASON_AMP','D20200224']\n",
    "#SUBPLOTS\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15,10]\n",
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "\n",
    "fontdict={'fontsize': 25,\n",
    "          'weight' : 'bold'}\n",
    "\n",
    "fontdicty={'fontsize': 10,\n",
    "          'weight' : 'bold',\n",
    "          'verticalalignment': 'baseline',\n",
    "          'horizontalalignment': 'center'}\n",
    "\n",
    "fontdictx={'fontsize': 10,\n",
    "          'weight' : 'bold',\n",
    "          'horizontalalignment': 'center'}\n",
    "\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "\n",
    "fig.suptitle('Attribute Quick View', fontsize=25,fontweight=\"bold\", color=\"black\", \n",
    "             position=(0.5,1.01))\n",
    "\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax1.scatter(dataSUBSETan['LONG'], dataSUBSETan['LAT'], c=dataSUBSETan['VEL'], s=10, cmap='viridis')\n",
    "ax1.set_title('VEL', fontdict=fontdict, color=\"green\")\n",
    "\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax2.scatter(dataSUBSETan['LONG'], dataSUBSETan['LAT'], c=dataSUBSETan['ACC'], s=10, cmap='viridis')\n",
    "ax2.set_title('ACC', fontdict=fontdict, color=\"orange\")\n",
    "\n",
    "ax3 = fig.add_subplot(223)\n",
    "ax3.scatter(dataSUBSETan['LONG'], dataSUBSETan['LAT'], c=dataSUBSETan['COHERENCE'], s=10, cmap='viridis')\n",
    "ax3.set_title('COHERENCE', fontdict=fontdict, color=\"brown\")\n",
    "\n",
    "\n",
    "ax4 = fig.add_subplot(224)\n",
    "ax4.scatter(dataSUBSETan['LONG'], dataSUBSETan['LAT'], c=dataSUBSETan['V_STDEV'], s=10, cmap='viridis')\n",
    "ax4.set_title(\"V_STDEV\", fontdict=fontdict, color=\"blue\")\n",
    "\n",
    "#CORRELATION MATRIX\n",
    "sns.pairplot(dataSUBSETan[cluster], kind='reg', diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "X = np.array(list(zip(dataSUBSETan['STD_DEF'], dataSUBSETan['VEL'],dataSUBSETan['ACC'],dataSUBSETan['COHERENCE'], dataSUBSETan['SEASON_AMP'])))\n",
    "#df_asc_north['ACC'], df_asc_north['V_STDEV'], df_asc_north['STD_DEF'])))\n",
    "# define the model & fit the model\n",
    "kmeans_model = KMeans(n_clusters=5, random_state=1).fit(X)\n",
    "# assign a cluster to each example\n",
    "yhat = kmeans_model.predict(X)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "                  \n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# All the program statements\n",
    "stop = timeit.default_timer()\n",
    "execution_time = stop - start\n",
    "\n",
    "print(\"Program Executed in \"+str(execution_time)) # It returns time in seconds\n",
    "\n",
    "#map the labels to colors\n",
    "c= ['b', 'r', 'y', 'g', 'c', 'm', 'e','f', 'u', 'd', 'a', 'h', 'i', 'j', 'k', 'l','n','o','p']\n",
    "colors = [c[i] for i in yhat]\n",
    "\n",
    "#Plot clusters with coordinates\n",
    "figure(num=None, figsize=(10, 8), dpi=100, facecolor='w', edgecolor='k')\n",
    "pyplot.scatter(dataSUBSETan['LONG'], dataSUBSETan['LAT'], c=yhat, s=3, cmap='viridis')\n",
    "plt.savefig('dataSUBSETan_kmeans_PERMIAN_STD_DEF_VEL_ACC_COHERENCE_SEASON_AMP_5.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MODEL EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#kmeans\n",
    "# define dataset\n",
    "X = np.array(list(zip(dataSUBSETan['VEL'],dataSUBSETan['D20200224'],dataSUBSETan['ACC'],dataSUBSETan['SEASON_AMP'])))\n",
    "# define the model & fit the model\n",
    "kmeans_model = KMeans(n_clusters=5, random_state=1).fit(X)\n",
    "# assign a cluster to each example\n",
    "yhat = kmeans_model.predict(X)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# All the program statements\n",
    "stop = timeit.default_timer()\n",
    "execution_time = stop - start\n",
    "\n",
    "print(\"Program Executed in \"+str(execution_time)) # It returns time in seconds\n",
    "\n",
    "#map the labels to colors\n",
    "c= ['b', 'r', 'y', 'g', 'c', 'm', 'e','f', 'u', 'd', 'a', 'h', 'i', 'j', 'k', 'l','n','o','p']\n",
    "colors = [c[i] for i in yhat]\n",
    "\n",
    "#Plot clusters with coordinates\n",
    "figure(num=None, figsize=(10, 8), dpi=100, facecolor='w', edgecolor='k')\n",
    "pyplot.scatter(dataSUBSETan['LONG'], dataSUBSETan['LAT'], c=yhat, s=2, cmap='viridis')\n",
    "plt.savefig('dataSUBSETan_kmeans_PERMIAN_all_no_QUALITY_5.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MiniBatch_Kmeans\n",
    "# define dataset\n",
    "X = np.array(list(zip(dataSUBSETan['VEL'],dataSUBSETan['D20200224'],dataSUBSETan['ACC'],dataSUBSETan['SEASON_AMP'])))\n",
    "# fit the model\n",
    "MiniBatch_model.fit(X)\n",
    "# assign a cluster to each example\n",
    "yhat = MiniBatch_model.predict(X)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "\n",
    "\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# All the program statements\n",
    "stop = timeit.default_timer()\n",
    "execution_time = stop - start\n",
    "\n",
    "print(\"Program Executed in \"+str(execution_time)) # It returns time in seconds\n",
    "\n",
    "#map the labels to colors\n",
    "c= ['b', 'r', 'y', 'g', 'c', 'm', 'e','f', 'u', 'd', 'a', 'h']\n",
    "colors = [c[i] for i in yhat]\n",
    "\n",
    "#Plot clusters with coordinates\n",
    "figure(num=None, figsize=(10, 8), dpi=100, facecolor='w', edgecolor='k')\n",
    "pyplot.scatter(dataSUBSETan['LONG'], dataSUBSETan['LAT'], c=yhat, s=2, cmap='viridis')\n",
    "plt.savefig('dataSUBSETan_MINIBATCH_PERMIAN_all_no_QUALITY_5.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#GMM\n",
    "# define dataset\n",
    "X = np.array(list(zip(dataSUBSETan['VEL'],dataSUBSETan['D20200224'],dataSUBSETan['ACC'],dataSUBSETan['SEASON_AMP'])))\n",
    "#define the model\n",
    "GMM_model = GMM(n_components=5)\n",
    "# fit the model\n",
    "GMM_model.fit(X)\n",
    "# assign a cluster to each example\n",
    "yhat = GMM_model.predict(X)\n",
    "# retrieve unique clusters\n",
    "GMM_clusters = unique(yhat)\n",
    "\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# All the program statements\n",
    "stop = timeit.default_timer()\n",
    "execution_time = stop - start\n",
    "\n",
    "print(\"Program Executed in \"+str(execution_time)) # It returns time in seconds\n",
    "\n",
    "#map the labels to colors\n",
    "c= ['b', 'r', 'y', 'g', 'c', 'm', 'e','f', 'u', 'd', 'a', 'h']\n",
    "colors = [c[i] for i in yhat]\n",
    "\n",
    "#Plot clusters with coordinates\n",
    "figure(num=None, figsize=(10, 8), dpi=100, facecolor='w', edgecolor='k')\n",
    "pyplot.scatter(dataSUBSETan['LONG'], dataSUBSETan['LAT'], c=yhat, s=2, cmap='viridis')\n",
    "plt.savefig('dataSUBSETan_GMM_PERMIAN_ALL_no_QUALITY_5.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#BIRCH\n",
    "X = np.array(list(zip(dataSUBSETan['VEL'],dataSUBSETan['D20200224'],dataSUBSETan['ACC'],dataSUBSETan['SEASON_AMP'])))\n",
    "# define the model\n",
    "Birch_model = Birch(threshold = 0.001, n_clusters=5)\n",
    "# fit the model\n",
    "Birch_model.fit(X)\n",
    "# assign a cluster to each example\n",
    "yhat = Birch_model.predict(X)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# All the program statements\n",
    "stop = timeit.default_timer()\n",
    "execution_time = stop - start\n",
    "\n",
    "print(\"Program Executed in \"+str(execution_time)) # It returns time in seconds\n",
    "\n",
    "#map the labels to colors\n",
    "c= ['b', 'r', 'y', 'g', 'c', 'm', 'e','f', 'u', 'd', 'a', 'h']\n",
    "colors = [c[i] for i in yhat]\n",
    "\n",
    "#Plot clusters with coordinates\n",
    "figure(num=None, figsize=(10, 8), dpi=100, facecolor='w', edgecolor='k')\n",
    "pyplot.scatter(dataSUBSETan['LONG'], dataSUBSETan['LAT'], c=yhat, s=2, cmap='viridis')\n",
    "plt.savefig('dataSUBSETan_BIRCH_PERMIAN_ALL_no_QUALITY_5.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MODEL ASSESSMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kmeans\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "labels = kmeans_model.labels_\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "\n",
    "# create scatter plot for samples from each cluster\n",
    "#for cluster in clusters:\n",
    "\t# get row indexes for samples with this cluster\n",
    "\t#row_ix = where(yhat == cluster)\n",
    "\t# create scatter of these samples\n",
    "\t#pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "# show the plot\n",
    "#pyplot.show()\n",
    "\n",
    "#print(\"Silhouette Coefficient: %0.3f\"\n",
    "      #% metrics.silhouette_score(X, labels, metric='sqeuclidean'))\n",
    "#Calinski-Harabasz Index\n",
    "print(\"Calinski Harabasz Score: %0.3f\"\n",
    "      % metrics.calinski_harabasz_score(X, labels))\n",
    "#Davies Bouldin Index\n",
    "print(\"Davies Bouldin Index: %0.3f\"\n",
    "      % metrics.davies_bouldin_score(X, labels))\n",
    "\n",
    "cluster_map = pd.DataFrame()\n",
    "cluster_map['data_index'] = dataSUBSETan.index.values\n",
    "cluster_map['cluster'] = kmeans_model.labels_\n",
    "\n",
    "cluster_map[cluster_map.cluster == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MINIBATCH kmeans\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "labels = MiniBatch_model.labels_\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "\n",
    "# create scatter plot for samples from each cluster\n",
    "#for cluster in clusters:\n",
    "\t# get row indexes for samples with this cluster\n",
    "\t#row_ix = where(yhat == cluster)\n",
    "\t# create scatter of these samples\n",
    "\t#pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "# show the plot\n",
    "#pyplot.show()\n",
    "\n",
    "#print(\"Silhouette Coefficient: %0.3f\"\n",
    "      #% metrics.silhouette_score(X, labels, metric='sqeuclidean'))\n",
    "#Calinski-Harabasz Index\n",
    "print(\"Calinski Harabasz Score: %0.3f\"\n",
    "      % metrics.calinski_harabasz_score(X, labels))\n",
    "#Davies Bouldin Index\n",
    "print(\"Davies Bouldin Index: %0.3f\"\n",
    "      % metrics.davies_bouldin_score(X, labels))\n",
    "\n",
    "cluster_map = pd.DataFrame()\n",
    "cluster_map['data_index'] = dataSUBSETan.index.values\n",
    "cluster_map['cluster'] = MiniBatch_model.labels_\n",
    "\n",
    "cluster_map[cluster_map.cluster == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GMM\n",
    "GMM_model.score()\n",
    "GMM_model.aic()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BIRCH\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "labels = Birch_model.labels_\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "\n",
    "# create scatter plot for samples from each cluster\n",
    "#for cluster in clusters:\n",
    "\t# get row indexes for samples with this cluster\n",
    "\t#row_ix = where(yhat == cluster)\n",
    "\t# create scatter of these samples\n",
    "\t#pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "# show the plot\n",
    "#pyplot.show()\n",
    "\n",
    "#print(\"Silhouette Coefficient: %0.3f\"\n",
    "      #% metrics.silhouette_score(X, labels, metric='sqeuclidean'))\n",
    "#Calinski-Harabasz Index\n",
    "print(\"Calinski Harabasz Score: %0.3f\"\n",
    "      % metrics.calinski_harabasz_score(X, labels))\n",
    "#Davies Bouldin Index\n",
    "print(\"Davies Bouldin Index: %0.3f\"\n",
    "      % metrics.davies_bouldin_score(X, labels))\n",
    "\n",
    "cluster_map = pd.DataFrame()\n",
    "cluster_map['data_index'] = dataSUBSETan.values\n",
    "cluster_map['cluster'] = Birch_model.labels_\n",
    "\n",
    "cluster_map[cluster_map.cluster == 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. EXPORT TO LAYER FOR ARCPRO: CSV TO SHP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('C:/599_Research/ARTIFICIAL/Permian/SA_SHP_OUTPUTS/asc_south_kmeans_PERMIAN_VEL_6_withlabels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MakeXYLayer.py\n",
    "# Description: Creates an XY layer and exports it to a layer file\n",
    "\n",
    "# import system modules \n",
    "import arcpy\n",
    "from arcpy import env\n",
    "\n",
    "# Set environment settings\n",
    "env.workspace = \"C:/599_Research/ARTIFICIAL/Permian/SA_SHP_OUTPUTS\"\n",
    " \n",
    "# Set the local variables\n",
    "in_Table = \"asc_south_kmeans_PERMIAN_VEL_6_withlabels.csv\"\n",
    "x_coords = \"LONG\"\n",
    "y_coords = \"LAT\"\n",
    "z_coords = \"HEIGHT\"\n",
    "out_Layer = \"asc_south_kmeans_PERMIAN_VEL_6_withlabels_layer\"\n",
    "saved_Layer = r\"C:/599_Research/ARTIFICIAL/Permian/SA_SHP_OUTPUTS/asc_south_kmeans_PERMIAN_VEL_6_MLOUTPUT.shp\"\n",
    " \n",
    "# Set the spatial reference\n",
    "spRef = arcpy.SpatialReference(4326)\n",
    " \n",
    "# Make the XY event layer...\n",
    "arcpy.MakeXYEventLayer_management(in_Table, x_coords, y_coords, out_Layer, spRef, z_coords)\n",
    " \n",
    "# Save to a layer file\n",
    "arcpy.SaveToLayerFile_management(out_Layer, saved_Layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
